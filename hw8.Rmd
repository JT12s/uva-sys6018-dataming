---
title: "Homework #8: Clustering" 
author: "**Jie Tang**"
date: "Due: Tue Apr 26 | 10:30am"
output: R6018::homework
---

**SYS 4582/6018 | Spring 2022 | University of Virginia **

*******************************************
```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation   
library(mclust)    # functions for mixture models
library(mixtools)  # poisregmixEM() function
```
:::


# Problem 1: Customer Segmentation with RFM (Recency, Frequency, and Monetary Value)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers. 


The data for this problem can be found here: <`r file.path(data.dir, "RFM.csv")`>. Cluster based on the Recency, Frequency, and Monetary value columns.


::: {.solution}
```{r}
data = read.csv('https://mdporter.github.io/SYS6018/data//RFM.csv')
```

:::

## a. Implement hierarchical clustering. 

- Describe any pre-processing steps you took (e.g., scaling, distance metric)
- State the linkage method you used with justification. 
- Show the resulting dendrogram
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
::: {.solution}
I chose euclidean method as distance calculation method, Ward's as linkage calculation method. With a scaling to mean = 0 sd= 1, we can make each variable with a same range. It can help us calculate a precise distance outcome.
With Ward's linkage, we calculate the increase in sum of squares. I think with this dissimilarity calculation, it is effective method for noisy data.
```{r}
X = dplyr::select(data, Recency, Frequency, Monetary)
X <- scale(X) #mean = 0 sd = 1
dX = dist(X, method='euclidean')
#Ward's Linkage as linkage method
hc = hclust(dX, method="ward.D") # ward's linkage
plot(hc, las=1, cex=.6) 
```

:::

```{r}

#By using ward's method,the gap of heights corresponds to the change in the Sum #of Squared Errors (SSE) for a merge
tibble(height = hc$height, K = row_number(-height)) %>%
ggplot(aes(K, height)) +
geom_line() +
geom_point(aes(color = ifelse(K == 8, "red", "black"))) +
scale_color_identity() +
coord_cartesian(xlim=c(1, 50))

#membership
member1 = cutree(hc, k=8)
#They are not in same cluster, when k = 8
member1[1]== member1[100]
```
We can look for the elbow point. From above figure, I picked cluster number equal to eight So, we have eight cluster.

## b. Implement k-means.  

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
::: {.solution}
```{r}
#With same scaling data from above, we scale data to mean=0 sd=1.
#By scaling the data (so each feature has the same variance), the resulting clustering can better adapt to the natural structure
#From above result, we estimate the clusters is eight.I set the kmax = 15. We can see whether elbow point is located in near point eight.
Kmax = 15
SSE = numeric(Kmax) 
for(k in 1:Kmax){
  km = kmeans(X, centers=k, nstart=25)    # use 25 starts
  SSE[k] = km$tot.withinss                # get SSE
}

#-- Plot results
plot(1:Kmax, SSE, type='o', las=1, xlab="K")
title("K-means for RFM data")

#We pick the elbow point, which seems like point six in graph.
km = kmeans(X, centers=6, nstart=25) # use K=6
#membership
member2 = km$cluster
#They are not same cluster, when k = 6.
member2[1] == member2[100]
```

:::

## c. Implement model-based clustering

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Describe the best model. What restrictions are on the shape of the components?
- Using your segmentation, are customers 1 and 100 in the same cluster?     


::: {.solution}
```{r}
#I still reuse the scaling from above data. mean = 0 sd = 1
library(mclust)
mix = Mclust(X)
summary(mix)   # finds 3 clusters

plot(mix, what="BIC")  
plot(mix, what="classification")
plot(mix, what="uncertainty")  
plot(mix, what="density")  

#-- get parameters
summary(mix, parameters=TRUE)


mix[["BIC"]]
```
c2and c3:From the summary of model, we can see the model with 7 componenets is the best. We can also see it from BIC figure. For mclust the best model is the one that maximizes their BIC. From the figure of BIC above, when the number of components is 7, BIC is the max.

```{r}
#c4
#membership
member3 = mix$classification
member3[1] == member3[100]
#customers 1 and 100 are not in same cluster.
```

:::

## d. Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others? 

::: {.solution}
If this task is part of my job, I think i will give a shot to all these three models to cluster customers. Based on the demand of companies, if they have labeled test data, we can try performance of each model and see which one perform the best. If there is no labeled test data, I will see aim of our clustering, like grouping customers who have higher chance to re-visit and consume again. We can see whether the cluster result fulfill this goal. In sum, based on the demand to pick our clustering model.

We cannot say one model is certainly better than the others. With a different aspect of testing the performance, any model can have the best performance. At the same time, those models have variance and even with a different scaling, the result will be totally different. So, we cannot say which one is the best.
:::



# Problem 2: Poisson Mixture Model (OPTIONAL: NOT GRADED)

The pmf of a Poisson random variable is:
\begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written:
\begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}



## a. What are the parameters of the model? 

::: {.solution}
Add Solution here
:::

## b. Write down the log-likelihood for $n$ independent observations ($x_1, x_2, \ldots, x_n$). 

::: {.solution}
Add Solution here
:::

## c. Suppose we have initial values of the parameters. Write down the equation for updating the *responsibilities*. 

::: {.solution} 
Add Solution here
:::


## d. Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters. 

::: {.solution}
Add Solution here
:::


## e. Fit a two-component Poisson mixture model, report the estimated parameter values, and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```


<div style="background-color:lightgrey; display: block; border-color: black; padding:1em">

Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of density estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`. 

Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.

</div>

::: {.solution}
Add Solution here
:::

## f. **2 pts Extra Credit**: Write a function that estimates this two-component Poisson mixture model using the EM approach. Show that it gives the same result as part *e*. 
- Note: you are not permitted to copy code.  Write everything from scratch and use comments to indicate how the code works (e.g., the E-step, M-step, initialization strategy, and convergence should be clear). 
- Cite any resources you consulted to help with the coding. 


::: {.solution}
Add Solution here
:::


